<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mukesh">
<meta name="dcterms.date" content="2025-06-18">

<title>Brush up your Machine learning Skill ‚Äì dataSherlock</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-e05b10a33b060cc5e8d9f624c63cd02d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">dataSherlock</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/MUKESHRAJMAHENDRAN"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/mukeshmahendran"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Brush up your Machine learning Skill</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Machine leaning</div>
                <div class="quarto-category">EDA</div>
                <div class="quarto-category">Analysis</div>
                <div class="quarto-category">Preprocessing</div>
                <div class="quarto-category">Prediction</div>
                <div class="quarto-category">Pipeline</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Mukesh </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 18, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Any sort of data science work <code>Look at your data directly even though you have an description</code>.</p>
</div>
</div>
<section id="machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning">Machine learning</h2>
<blockquote class="blockquote">
<p>In a simple terms computer is able to learn from the data without being explicitly programmed.</p>
</blockquote>
<section id="types-of-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="types-of-machine-learning">Types of Machine Learning</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 44%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>Type</th>
<th>Description</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Supervised Learning</strong></td>
<td>Uses labeled data (input + output)</td>
<td>Classification, Regression</td>
</tr>
<tr class="even">
<td><strong>Unsupervised Learning</strong></td>
<td>Uses unlabeled data (only input)</td>
<td>Clustering, Dimensionality Reduction</td>
</tr>
<tr class="odd">
<td><strong>Semi-supervised Learning</strong></td>
<td>Mix of labeled and unlabeled data</td>
<td>Image classification with limited labels</td>
</tr>
<tr class="even">
<td><strong>Reinforcement Learning</strong></td>
<td>Agent learns by trial and error with rewards/penalties</td>
<td>Game AI, Robotics</td>
</tr>
</tbody>
</table>
</section>
<section id="key-concepts-in-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="key-concepts-in-machine-learning">Key Concepts in Machine Learning</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Features (X) or Independent variable</strong></td>
<td>Input variables used to make predictions</td>
</tr>
<tr class="even">
<td><strong>Target (y) or Dependent variable</strong></td>
<td>Output variable we want to predict</td>
</tr>
<tr class="odd">
<td><strong>Training Data</strong></td>
<td>Data used to train the model</td>
</tr>
<tr class="even">
<td><strong>Test Data</strong></td>
<td>Data used to evaluate the model</td>
</tr>
<tr class="odd">
<td><strong>Overfitting</strong></td>
<td>Model performs well on training data but poorly on new data</td>
</tr>
<tr class="even">
<td><strong>Underfitting</strong></td>
<td>Model doesn‚Äôt perform well even on training data</td>
</tr>
<tr class="odd">
<td><strong>Bias-Variance Tradeoff</strong></td>
<td>Balancing simplicity vs complexity of a model</td>
</tr>
<tr class="even">
<td><strong>Generalization</strong></td>
<td>How well the model performs on unseen data</td>
</tr>
</tbody>
</table>
</section>
<section id="data-understanding-preprocessing-steps" class="level3">
<h3 class="anchored" data-anchor-id="data-understanding-preprocessing-steps">Data Understanding &amp; Preprocessing Steps</h3>
<p>When I first receive the dataset, I begin by performing an initial assessment to understand its structure and quality. This involves the following key steps:</p>
<section id="data-inspection" class="level4">
<h4 class="anchored" data-anchor-id="data-inspection">1. Data Inspection</h4>
<ul>
<li><strong>Check for Missing Values:</strong> Identify any missing or null entries across all features. Depending on the volume and pattern of missingness, decide whether to drop, impute(replace), or model around them.</li>
<li><strong>Data Types Check:</strong> Review the data types of each feature (e.g., integer, float, object) to ensure they align with the expected format. For example, date columns should be in datetime format, and categorical variables should be strings or category types.</li>
<li><strong>Correlation Analysis:</strong> Examine the correlation between independent variables and the target variable. This helps identify potentially predictive features and detect multicollinearity issues.</li>
</ul>
</section>
<section id="feature-exploration-engineering" class="level4">
<h4 class="anchored" data-anchor-id="feature-exploration-engineering">2. Feature Exploration &amp; Engineering</h4>
<ul>
<li><p><strong>Date Handling:</strong> If the dataset includes date-time fields, I extract meaningful features such as year, month, day, week of year, day of week, etc. These engineered features can significantly enhance model performance by capturing temporal patterns.</p></li>
<li><p><strong>Missing Value Treatment:</strong></p>
<ul>
<li>Numerical Features|continous features: Impute missing values using mean, median, or advanced methods like KNN or model-based imputation.</li>
<li>Categorical Features: Replace missing values with ‚ÄúUnknown‚Äù or use frequency-based approaches.</li>
</ul></li>
<li><p><strong>Normalization / Scaling:</strong> Apply normalization or standardization techniques to numerical features when working with models sensitive to feature scales (e.g., SVMs, neural networks, gradient descent-based algorithms).</p></li>
<li><p><strong>Categorification (Encoding):</strong> Convert categorical variables into numerical formats suitable for machine learning models:</p>
<ul>
<li><strong>One-Hot Encoding:</strong> Preferred for nominal categorical variables with no inherent order (especially when cardinality is low).</li>
<li><strong>Label Encoding:</strong> Used for binary or ordinal variables where natural ordering exists. eg: <code>["small", "medium", "hard"]</code></li>
<li><strong>Target Encoding / Leave-One-Out Encoding:</strong> Useful for high-cardinality categorical features (like zip codes), where one-hot encoding would lead to dimensionality explosion.(Leave-One-Out Encoding replaces each category with the average target value of all other rows in the same category, excluding the current row. This helps prevent overfitting by avoiding data leakage during encoding.)</li>
</ul></li>
</ul>
</section>
<section id="categorical-vs-continuous-variables" class="level4">
<h4 class="anchored" data-anchor-id="categorical-vs-continuous-variables">3. Categorical vs Continuous Variables</h4>
<ul>
<li><strong>Categorical Variables:</strong> These typically have object or string data types and represent discrete categories. They may require encoding before modeling.</li>
<li><strong>Continuous Variables:</strong> These are numeric features that can take any value within a range. These usually undergo scaling or binning(discrete intervals or bins) depending on the model requirements.</li>
</ul>
</section>
<section id="cardinality-consideration" class="level4">
<h4 class="anchored" data-anchor-id="cardinality-consideration">4. Cardinality Consideration</h4>
<ul>
<li><strong>High Cardinality:</strong> Features like <code>zip_code</code>, <code>user_id</code>, or <code>product_id</code> that have thousands of unique values. These need special handling‚Äîsuch as grouping rare categories, hashing, or using embedding layers in deep learning.</li>
<li><strong>Low Cardinality:</strong> Features with a small number of distinct values. These are generally easier to encode using one-hot or label encoding methods.</li>
</ul>
</section>
<section id="ordinal-variables" class="level4">
<h4 class="anchored" data-anchor-id="ordinal-variables">5. Ordinal Variables</h4>
<ul>
<li>Some categorical variables have a natural order, such as <code>"easy"</code>, <code>"medium"</code>, <code>"hard"</code> or <code>"low"</code>, <code>"medium"</code>, <code>"high"</code>. These are called <strong>ordinal variables</strong>, and their order must be preserved during encoding (e.g., via custom mapping or ordinal encoding).</li>
</ul>
</section>
</section>
<section id="decision-trees" class="level3">
<h3 class="anchored" data-anchor-id="decision-trees">üå≥ Decision Trees</h3>
</section>
<section id="what-is-a-decision-tree" class="level3">
<h3 class="anchored" data-anchor-id="what-is-a-decision-tree">What is a Decision Tree?</h3>
<p>A <strong>decision tree</strong> is a machine learning model that makes predictions by learning simple decision rules inferred from the data features. It splits the data into subsets based on feature values, forming a tree-like structure where:</p>
<ul>
<li>Each <strong>internal node</strong> represents a test on a feature.</li>
<li>Each <strong>branch</strong> represents the outcome of that test.</li>
<li>Each <strong>leaf node</strong> represents a final prediction or class label (in classification) or value (in regression).</li>
</ul>
</section>
<section id="how-does-a-decision-tree-work" class="level3">
<h3 class="anchored" data-anchor-id="how-does-a-decision-tree-work">How Does a Decision Tree Work?</h3>
<ol type="1">
<li><strong>Recursive Partitioning</strong>: The algorithm starts at the root of the tree and tries to split the data into two groups such that the resulting groups are as ‚Äúpure‚Äù as possible with respect to the target variable.</li>
<li><strong>Splitting Criteria</strong>:
<ul>
<li>For <strong>regression trees</strong>, it uses metrics like <strong>Mean Squared Error (MSE)</strong> or <strong>Mean Absolute Error (MAE)</strong>.</li>
<li>For <strong>classification trees</strong>, it uses measures like <strong>Gini impurity</strong> or <strong>Information Gain (based on entropy)</strong>.</li>
<li>(<strong>Gini impurity</strong> is a measure of how often a randomly chosen element from a set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in that subset).</li>
<li>(<strong>Information Gain</strong> (IG) measures how much ‚Äúinformation‚Äù a feature provides about the class labels. In other words, it tells us how well a feature separates the data into classes.)</li>
</ul></li>
<li>This process continues recursively until a stopping criterion is met (e.g., maximum depth, minimum number of samples per leaf).</li>
</ol>
</section>
<section id="pros-cons-of-decision-trees" class="level3">
<h3 class="anchored" data-anchor-id="pros-cons-of-decision-trees">Pros &amp; Cons of Decision Trees</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Easy to interpret and visualize</td>
<td>Prone to overfitting</td>
</tr>
<tr class="even">
<td>Handles both numerical and categorical data well</td>
<td>Unstable: small changes in data can lead to very different trees</td>
</tr>
<tr class="odd">
<td>No need for feature scaling</td>
<td>May not generalize well</td>
</tr>
</tbody>
</table>
</section>
<section id="ensemble-learning-bagging-random-forests" class="level3">
<h3 class="anchored" data-anchor-id="ensemble-learning-bagging-random-forests">Ensemble Learning: Bagging &amp; Random Forests</h3>
</section>
<section id="what-is-an-ensemble-method" class="level3">
<h3 class="anchored" data-anchor-id="what-is-an-ensemble-method">What is an Ensemble Method?</h3>
<p>An <strong>ensemble method</strong> combines multiple models to improve performance and reduce variance and bias.</p>
</section>
<section id="bagging-bootstrap-aggregating" class="level3">
<h3 class="anchored" data-anchor-id="bagging-bootstrap-aggregating">üß∫ Bagging (Bootstrap Aggregating)</h3>
<p><strong>Bagging</strong> works by:</p>
<ol type="1">
<li>Randomly sampling subsets of the training data <strong>with replacement</strong> (called bootstrap samples).</li>
<li>Training a base model (usually a decision tree) on each subset.</li>
<li>Averaging predictions (for regression) or using majority voting (for classification) across all models.</li>
</ol>
<blockquote class="blockquote">
<p>This reduces variance and helps prevent overfitting.</p>
</blockquote>
</section>
<section id="random-forests" class="level3">
<h3 class="anchored" data-anchor-id="random-forests">üå≤ Random Forests</h3>
<p><strong>Random Forests</strong> are an extension of bagging with one key difference:</p>
<ul>
<li>At each split in the tree, only a <strong>random subset of features</strong> is considered for splitting.</li>
</ul>
<p>This further <strong>decorrelates</strong> the trees, improving performance.</p>
<section id="why-are-random-forests-effective" class="level4">
<h4 class="anchored" data-anchor-id="why-are-random-forests-effective">Why Are Random Forests Effective?</h4>
<ul>
<li>Each tree sees slightly different data and features.</li>
<li>Errors from individual trees tend to cancel out when averaged.</li>
<li>They‚Äôre robust to noise, outliers, and overfitting.</li>
</ul>
</section>
</section>
</section>
<section id="out-of-bag-oob-error" class="level2">
<h2 class="anchored" data-anchor-id="out-of-bag-oob-error">Out-of-Bag (OOB) Error</h2>
<section id="what-is-oob-error" class="level3">
<h3 class="anchored" data-anchor-id="what-is-oob-error">What is OOB Error?</h3>
<p>In random forests, since each tree is trained on a bootstrapped sample (~63% of the data), the remaining ~37% of rows <strong>not used in training</strong> a particular tree are called <strong>Out-of-Bag (OOB)</strong> samples.</p>
</section>
<section id="why-is-oob-error-useful" class="level3">
<h3 class="anchored" data-anchor-id="why-is-oob-error-useful">Why Is OOB Error Useful?</h3>
<ul>
<li>You can use these OOB samples to estimate how well the model performs <strong>without needing a separate validation set</strong>.</li>
<li>For each row, you average predictions only from trees that did <strong>not</strong> include that row during training.</li>
</ul>
<p>üß† <strong>Intuition</strong>: OOB error is like having a built-in cross-validation mechanism for random forests.</p>
</section>
</section>
<section id="model-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="model-interpretation">Model Interpretation</h2>
<p>Even though ensemble models like random forests are more complex than single decision trees, they still allow us to understand <strong>how</strong> and <strong>why</strong> they make their predictions.</p>
<section id="feature-importance" class="level3">
<h3 class="anchored" data-anchor-id="feature-importance">1. <strong>Feature Importance</strong></h3>
<ul>
<li>Measures how much each feature contributes to reducing uncertainty (or error) in predictions.</li>
<li>Computed by averaging the reduction in impurity (like Gini or MSE) brought by each feature across all trees.</li>
</ul>
<p>üéØ Use Case: Identify which features are most useful‚Äîdrop irrelevant ones or focus on them for domain insights.</p>
</section>
<section id="finding-out-of-domain-data" class="level3">
<h3 class="anchored" data-anchor-id="finding-out-of-domain-data">2. <strong>Finding Out-of-Domain Data</strong></h3>
<ul>
<li>Train a classifier to distinguish between training and test sets.</li>
<li>If the classifier performs well, it means the test set differs significantly from the training set.</li>
<li>Helps identify <strong>distribution shifts</strong> that may affect generalization.</li>
</ul>
</section>
<section id="partial-dependence-plots-pdp" class="level3">
<h3 class="anchored" data-anchor-id="partial-dependence-plots-pdp">3. <strong>Partial Dependence Plots (PDP)</strong></h3>
<ul>
<li>Shows how a feature affects predictions <strong>on average</strong>, holding other features constant.</li>
<li>Reveals non-linear relationships and interactions.</li>
</ul>
</section>
<section id="individual-conditional-expectation-ice-plots" class="level3">
<h3 class="anchored" data-anchor-id="individual-conditional-expectation-ice-plots">4. <strong>Individual Conditional Expectation (ICE) Plots</strong></h3>
<ul>
<li>Like PDP but shows the effect for <strong>individual data points</strong> rather than averages.</li>
<li>Helps detect heterogeneity in effects.</li>
</ul>
</section>
<section id="tree-interpreter-shap-values" class="level3">
<h3 class="anchored" data-anchor-id="tree-interpreter-shap-values">5. <strong>Tree Interpreter / SHAP Values</strong></h3>
<ul>
<li>Explains individual predictions by decomposing the contribution of each feature.</li>
<li>Uses techniques like <strong>SHapley Additive exPlanations (SHAP)</strong> to fairly attribute prediction changes to input features.</li>
</ul>
<p>üéØ Use Case: Explain why a specific prediction was made‚Äîfor transparency, fairness, or debugging.</p>
</section>
<section id="ensembling-techniques" class="level3">
<h3 class="anchored" data-anchor-id="ensembling-techniques">Ensembling Techniques</h3>
</section>
<section id="bagging-as-discussed-above" class="level3">
<h3 class="anchored" data-anchor-id="bagging-as-discussed-above">1. <strong>Bagging (as discussed above)</strong></h3>
<ul>
<li>Combines many models trained on different subsets of data.</li>
<li>Reduces variance ‚Üí improves generalization.</li>
</ul>
</section>
<section id="boosting" class="level3">
<h3 class="anchored" data-anchor-id="boosting">2. <strong>Boosting</strong></h3>
<ul>
<li>Sequentially trains weak learners, each correcting the errors of its predecessor.</li>
<li>Final prediction is a <strong>weighted sum</strong> of all models‚Äô predictions.</li>
</ul>
<section id="popular-boosting-algorithms" class="level4">
<h4 class="anchored" data-anchor-id="popular-boosting-algorithms">Popular Boosting Algorithms:</h4>
<ul>
<li><strong>AdaBoost</strong></li>
<li><strong>Gradient Boosted Trees (GBDT)</strong></li>
<li><strong>XGBoost</strong>, <strong>LightGBM</strong>, <strong>CatBoost</strong></li>
</ul>
</section>
<section id="key-idea-behind-boosting" class="level4">
<h4 class="anchored" data-anchor-id="key-idea-behind-boosting">Key Idea Behind Boosting:</h4>
<ol type="1">
<li>Train a simple model (often shallow trees).</li>
<li>Compute residuals (errors).</li>
<li>Train a new model to predict those residuals.</li>
<li>Repeat and add corrections iteratively.</li>
</ol>
<p>üß† <strong>Intuition</strong>: Boosting learns slowly and focuses on hard-to-predict cases.</p>
</section>
</section>
<section id="bagging-vs.-boosting" class="level3">
<h3 class="anchored" data-anchor-id="bagging-vs.-boosting">Bagging vs.&nbsp;Boosting</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Feature</th>
<th>Bagging (e.g., Random Forest)</th>
<th>Boosting (e.g., XGBoost)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Training Style</td>
<td>Parallel</td>
<td>Sequential</td>
</tr>
<tr class="even">
<td>Focus</td>
<td>Reduce variance</td>
<td>Reduce bias</td>
</tr>
<tr class="odd">
<td>Overfitting</td>
<td>Less prone</td>
<td>More prone if not tuned</td>
</tr>
<tr class="even">
<td>Speed</td>
<td>Faster to train</td>
<td>Slower due to sequential steps</td>
</tr>
<tr class="odd">
<td>Performance</td>
<td>Strong baseline</td>
<td>Often higher accuracy with tuning</td>
</tr>
</tbody>
</table>
</section>
<section id="summary-when-to-use-what" class="level3">
<h3 class="anchored" data-anchor-id="summary-when-to-use-what">Summary: When to Use What?</h3>
<ul>
<li><strong>Use Decision Trees</strong> for interpretable, fast baselines.</li>
<li><strong>Use Random Forests (Bagging)</strong> when you want good performance with less tuning.</li>
<li><strong>Use Boosting</strong> when you want high performance and can afford more tuning and time.</li>
<li><strong>Use OOB error</strong> to validate without a separate validation set.</li>
<li><strong>Use Feature Importance / SHAP / PDP</strong> to understand and explain your model.</li>
</ul>
</section>
<section id="model-evaluation-metrics" class="level3">
<h3 class="anchored" data-anchor-id="model-evaluation-metrics">Model Evaluation Metrics</h3>
<p>Model evaluation is the process of assessing how well a machine learning model performs on unseen data. There are different metrics for classification and regression tasks.</p>
</section>
<section id="classification-metrics" class="level3">
<h3 class="anchored" data-anchor-id="classification-metrics">‚úÖ Classification Metrics</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 25%">
<col style="width: 41%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th>Metric</th>
<th>Description</th>
<th>Formula / Use Case</th>
<th>Function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Accuracy</strong></td>
<td>% of total correct predictions (both true positives and true negatives)</td>
<td><code>TP + TN / TP + FP + TN + FN</code></td>
<td><code>accuracy_score()</code></td>
</tr>
<tr class="even">
<td><strong>Precision</strong></td>
<td>How many selected items are relevant?</td>
<td><code>TP / (TP + FP)</code></td>
<td><code>precision_score()</code></td>
</tr>
<tr class="odd">
<td><strong>Recall (Sensitivity)</strong></td>
<td>How many relevant items were selected?</td>
<td><code>TP / (TP + FN)</code></td>
<td><code>recall_score()</code></td>
</tr>
<tr class="even">
<td><strong>F1 Score</strong></td>
<td>Harmonic mean of precision and recall</td>
<td><code>2 * (precision * recall)/(precision + recall)</code></td>
<td><code>f1_score()</code></td>
</tr>
<tr class="odd">
<td><strong>Confusion Matrix</strong></td>
<td>Table showing counts of true positives, false positives, true negatives, false negatives</td>
<td>Visual summary of performance</td>
<td><code>confusion_matrix()</code></td>
</tr>
<tr class="even">
<td><strong>ROC-AUC</strong></td>
<td>Area under the ROC curve; measures model‚Äôs ability to distinguish between classes</td>
<td>Higher AUC = better performance</td>
<td><code>roc_auc_score()</code></td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>üìå Use <strong>ROC-AUC</strong> when dealing with imbalanced datasets. For class imbalance, prefer <strong>F1 score</strong> over accuracy.</p>
</div>
</div>
</section>
<section id="regression-metrics" class="level3">
<h3 class="anchored" data-anchor-id="regression-metrics">üìà Regression Metrics</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 25%">
<col style="width: 41%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th>Metric</th>
<th>Description</th>
<th>Formula / Use Case</th>
<th>Function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Mean Absolute Error (MAE)</strong></td>
<td>Average of absolute errors</td>
<td>Mean of</td>
<td>actual - predicted</td>
</tr>
<tr class="even">
<td><strong>Mean Squared Error (MSE)</strong></td>
<td>Average of squared errors</td>
<td>Mean of (actual - predicted)¬≤</td>
<td><code>mean_squared_error()</code></td>
</tr>
<tr class="odd">
<td><strong>R¬≤ Score (R-squared)</strong></td>
<td>Proportion of variance explained by the model</td>
<td>Best possible score is 1.0</td>
<td><code>r2_score()</code></td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p>üìå MAE is more interpretable; MSE penalizes large errors more heavily.<br>
R¬≤ tells how well your model fits the data compared to a baseline.</p>
</blockquote>
</section>
<section id="hyperparameter-tuning" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameter-tuning">üîß Hyperparameter Tuning</h3>
<p>Hyperparameters are settings that control how models learn. Unlike model parameters (like weights), hyperparameters are set before training.</p>
</section>
<section id="techniques" class="level3">
<h3 class="anchored" data-anchor-id="techniques">Techniques:</h3>
<ol type="1">
<li><strong>Grid Search</strong>
<ul>
<li>Tries every combination of given hyperparameters.</li>
<li>Exhaustive but slow if too many parameters.</li>
<li>Use: When you have few parameters or want exhaustive search.</li>
</ul></li>
<li><strong>Random Search</strong>
<ul>
<li>Randomly samples combinations from specified ranges.</li>
<li>Often faster than Grid Search and sometimes finds better results.</li>
</ul></li>
<li><strong>Bayesian Optimization</strong>
<ul>
<li>Uses probabilistic models to choose next parameter set.</li>
<li>Efficient for high-dimensional spaces.</li>
<li>Requires external libraries like <code>scikit-optimize</code>.</li>
</ul></li>
</ol>
</section>
<section id="cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="cross-validation">üîÅ 12. Cross-validation</h3>
<p>Cross-validation helps estimate how well your model will perform on unseen data by evaluating it on multiple random splits of the dataset.</p>
</section>
<section id="k-fold-cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="k-fold-cross-validation">K-Fold Cross-Validation:</h3>
<ul>
<li>Splits data into K parts (folds).</li>
<li>Trains on K-1 folds, tests on 1 fold ‚Äî repeats K times.</li>
</ul>
</section>
<section id="ensemble-methods" class="level3">
<h3 class="anchored" data-anchor-id="ensemble-methods">üß† 13. Ensemble Methods</h3>
<p>Ensemble methods combine predictions from multiple models to improve performance.</p>
</section>
<section id="types-of-ensemble-learning" class="level3">
<h3 class="anchored" data-anchor-id="types-of-ensemble-learning">Types of Ensemble Learning:</h3>
<section id="bagging-bootstrap-aggregating-1" class="level4">
<h4 class="anchored" data-anchor-id="bagging-bootstrap-aggregating-1">1. <strong>Bagging (Bootstrap Aggregating)</strong></h4>
<ul>
<li>Builds multiple models on bootstrapped samples of the data.</li>
<li>Final prediction is average (regression) or majority vote (classification).</li>
<li>Reduces variance.</li>
<li>Example: <code>BaggingClassifier()</code>, <code>RandomForestClassifier()</code></li>
</ul>
</section>
<section id="boosting-1" class="level4">
<h4 class="anchored" data-anchor-id="boosting-1">2. <strong>Boosting</strong></h4>
<ul>
<li>Sequentially trains models to correct errors of previous models.</li>
<li>Focuses on hard-to-predict instances.</li>
<li>Reduces bias.</li>
<li>Examples:
<ul>
<li><code>AdaBoostClassifier()</code></li>
<li><code>GradientBoostingClassifier()</code></li>
<li>Popular: <code>XGBoost</code>, <code>LightGBM</code>, <code>CatBoost</code></li>
</ul></li>
</ul>
</section>
<section id="voting" class="level4">
<h4 class="anchored" data-anchor-id="voting">3. <strong>Voting</strong></h4>
<ul>
<li>Combines predictions from multiple base classifiers.</li>
<li>Hard Voting: Majority class.</li>
<li>Soft Voting: Weighted probabilities.</li>
</ul>
</section>
<section id="stacking" class="level4">
<h4 class="anchored" data-anchor-id="stacking">4. <strong>Stacking</strong></h4>
<ul>
<li>Train a ‚Äúmeta-model‚Äù to combine outputs of base models.</li>
<li>Base models‚Äô predictions become new features for the meta-model.</li>
</ul>
</section>
</section>
</section>
<section id="summary-table" class="level2">
<h2 class="anchored" data-anchor-id="summary-table">Summary Table</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 60%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th>Topic</th>
<th>Key Functions/Classes</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Model Evaluation</td>
<td><code>accuracy_score</code>, <code>precision_score</code>, etc.</td>
<td>Assess model performance</td>
</tr>
<tr class="even">
<td>Hyperparameter Tuning</td>
<td><code>GridSearchCV</code>, <code>RandomizedSearchCV</code></td>
<td>Find best model settings</td>
</tr>
<tr class="odd">
<td>Feature Engineering</td>
<td><code>StandardScaler</code>, <code>OneHotEncoder</code>, <code>SimpleImputer</code></td>
<td>Improve input quality</td>
</tr>
<tr class="even">
<td>Pipelines</td>
<td><code>Pipeline</code></td>
<td>Automate preprocessing + modeling</td>
</tr>
<tr class="odd">
<td>Cross-validation</td>
<td><code>cross_val_score</code></td>
<td>Estimate generalization performance</td>
</tr>
<tr class="even">
<td>Ensemble Methods</td>
<td><code>BaggingClassifier</code>, <code>GradientBoostingClassifier</code>, <code>VotingClassifier</code>, <code>StackingClassifier</code></td>
<td>Boost performance via combining models</td>
</tr>
</tbody>
</table>
<section id="what-is-clustering" class="level3">
<h3 class="anchored" data-anchor-id="what-is-clustering">What is Clustering?</h3>
<p><strong>Clustering</strong> is an <strong>unsupervised machine learning technique</strong> used to group similar data points together into clusters. It helps find patterns or structures in data <strong>without prior knowledge of the groups</strong> ‚Äî unlike classification, where we already know the categories.</p>
</section>
<section id="why-do-we-use-clustering" class="level3">
<h3 class="anchored" data-anchor-id="why-do-we-use-clustering">Why Do We Use Clustering?</h3>
<p>Clustering helps answer questions like:</p>
<ul>
<li>How many distinct groups exist in my data?</li>
<li>Are there any unusual or outlier observations?</li>
<li>Can I simplify or summarize the data by grouping similar items?</li>
</ul>
<p>Common use cases include: - Customer segmentation - Image compression - Document grouping - Anomaly detection - Social network analysis</p>
</section>
<section id="types-of-clustering-algorithms" class="level3">
<h3 class="anchored" data-anchor-id="types-of-clustering-algorithms">Types of Clustering Algorithms</h3>
<p>Here are some popular clustering algorithms:</p>
<section id="k-means-clustering" class="level4">
<h4 class="anchored" data-anchor-id="k-means-clustering">1. <strong>K-Means Clustering</strong></h4>
<ul>
<li>Groups data into <code>k</code> number of clusters.</li>
<li>Starts with random centers and iteratively assigns points to the nearest cluster center.</li>
<li>Goal: Minimize the sum of squared distances between points and their cluster centers.</li>
</ul>
<p>‚úÖ Pros: Simple, fast<br>
‚ùå Cons: Needs <code>k</code> specified, assumes spherical clusters</p>
</section>
<section id="hierarchical-clustering" class="level4">
<h4 class="anchored" data-anchor-id="hierarchical-clustering">2. <strong>Hierarchical Clustering</strong></h4>
<ul>
<li>Builds a tree-like structure (dendrogram) showing how clusters merge or split.</li>
<li>Two types:
<ul>
<li><strong>Agglomerative</strong> (bottom-up): starts with individual points, merges them.</li>
<li><strong>Divisive</strong> (top-down): starts with all points in one cluster, splits recursively.</li>
</ul></li>
</ul>
<p>‚úÖ Pros: No need to specify number of clusters<br>
‚ùå Cons: Computationally expensive for large datasets</p>
</section>
<section id="dbscan-density-based-spatial-clustering-of-applications-with-noise" class="level4">
<h4 class="anchored" data-anchor-id="dbscan-density-based-spatial-clustering-of-applications-with-noise">3. <strong>DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</strong></h4>
<ul>
<li>Groups points that are close together and marks outliers as noise.</li>
<li>Doesn‚Äôt require specifying number of clusters.</li>
</ul>
<p>‚úÖ Pros: Handles noise well, finds arbitrarily shaped clusters<br>
‚ùå Cons: Sensitive to parameter settings</p>
</section>
<section id="gaussian-mixture-models-gmms" class="level4">
<h4 class="anchored" data-anchor-id="gaussian-mixture-models-gmms">4. <strong>Gaussian Mixture Models (GMMs)</strong></h4>
<ul>
<li>Assumes data comes from a mixture of Gaussian distributions.</li>
<li>Uses probabilities to assign points to clusters.</li>
</ul>
<p>‚úÖ Pros: Soft clustering (gives probability of belonging to each cluster)<br>
‚ùå Cons: Slower than K-Means</p>
</section>
</section>
<section id="how-does-clustering-work-simplified" class="level3">
<h3 class="anchored" data-anchor-id="how-does-clustering-work-simplified">How Does Clustering Work? (Simplified)</h3>
<p>Let‚Äôs take <strong>K-Means</strong> as an example:</p>
<ol type="1">
<li>Choose the number of clusters <code>k</code>.</li>
<li>Randomly place <code>k</code> centroids (center points).</li>
<li>Assign each data point to the nearest centroid.</li>
<li>Recalculate centroids based on the mean of assigned points.</li>
<li>Repeat steps 3‚Äì4 until centroids stabilize.</li>
</ol>
</section>
<section id="evaluating-clusters" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-clusters">Evaluating Clusters</h3>
<p>There‚Äôs no ‚Äúright‚Äù answer in unsupervised learning, but you can still evaluate quality using metrics like:</p>
<ul>
<li><strong>Silhouette Score</strong>: Measures how similar a point is to its own cluster vs others (ranges from -1 to +1)</li>
<li><strong>Elbow Method</strong>: Helps choose optimal <code>k</code> by plotting sum of squared distances vs.&nbsp;number of clusters</li>
<li><strong>Davies-Bouldin Index</strong>, <strong>Calinski-Harabasz Index</strong>, etc.</li>
</ul>
</section>
<section id="real-life-example" class="level3">
<h3 class="anchored" data-anchor-id="real-life-example">Real-Life Example</h3>
<p>Imagine you‚Äôre a marketing analyst and want to segment customers based on spending habits:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Customer</th>
<th>Annual Income</th>
<th>Spending Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>50</td>
<td>40</td>
</tr>
<tr class="even">
<td>B</td>
<td>60</td>
<td>60</td>
</tr>
<tr class="odd">
<td>C</td>
<td>10</td>
<td>90</td>
</tr>
<tr class="even">
<td>D</td>
<td>80</td>
<td>20</td>
</tr>
</tbody>
</table>
<p>Using clustering, you might discover: - Group 1: High income, high spenders - Group 2: Low income, high spenders - Group 3: High income, low spenders</p>
<p>Each group can be targeted with different marketing strategies.</p>
</section>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Feature</th>
<th>Clustering</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Type</td>
<td>Unsupervised</td>
</tr>
<tr class="even">
<td>Input</td>
<td>Data without labels</td>
</tr>
<tr class="odd">
<td>Output</td>
<td>Groups/clusters of similar points</td>
</tr>
<tr class="even">
<td>Popular Algorithms</td>
<td>K-Means, DBSCAN, Hierarchical</td>
</tr>
<tr class="odd">
<td>Evaluation Metrics</td>
<td>Silhouette score, Elbow method</td>
</tr>
</tbody>
</table>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/MUKESHRAJMAHENDRAN\.github\.io\/datasherlock\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>