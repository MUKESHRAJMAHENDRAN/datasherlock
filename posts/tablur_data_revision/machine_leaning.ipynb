{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08df32ad-b0ae-45f2-b544-886682742a12",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Brush up your Machine learning Skill\"\n",
    "author: \"Mukesh\"\n",
    "date: \"2025-06-18\"\n",
    "categories: [Machine leaning, EDA, Analysis, Preprocessing, Prediction, Pipeline]\n",
    "image: \"machine_learning.jpeg\"\n",
    "draft: False\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e17e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Machine learning\n",
    "> In a simple terms computer is able to learn from the data without being explicitly programmed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d11f8-fcdd-4d16-b05f-031ee5563645",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "Any sort of data science work `Look at your data directly even though you have an description`.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b79b1f",
   "metadata": {},
   "source": [
    "### Types of Machine Learning\n",
    "\n",
    "| Type | Description | Examples |\n",
    "|------|-------------|----------|\n",
    "| **Supervised Learning** | Uses labeled data (input + output) | Classification, Regression |\n",
    "| **Unsupervised Learning** | Uses unlabeled data (only input) | Clustering, Dimensionality Reduction |\n",
    "| **Semi-supervised Learning** | Mix of labeled and unlabeled data | Image classification with limited labels |\n",
    "| **Reinforcement Learning** | Agent learns by trial and error with rewards/penalties | Game AI, Robotics |\n",
    "\n",
    "### Key Concepts in Machine Learning\n",
    "\n",
    "| Concept | Explanation |\n",
    "|--------|-------------|\n",
    "| **Features (X) or Independent variable** | Input variables used to make predictions |\n",
    "| **Target (y) or Dependent variable** | Output variable we want to predict |\n",
    "| **Training Data** | Data used to train the model |\n",
    "| **Test Data** | Data used to evaluate the model |\n",
    "| **Overfitting** | Model performs well on training data but poorly on new data |\n",
    "| **Underfitting** | Model doesn't perform well even on training data |\n",
    "| **Bias-Variance Tradeoff** | Balancing simplicity vs complexity of a model |\n",
    "| **Generalization** | How well the model performs on unseen data |\n",
    "\n",
    "### Data Understanding & Preprocessing\n",
    "\n",
    "When I first receive the dataset, I begin by performing an initial assessment to understand its structure and quality. This involves the following key steps:\n",
    "\n",
    "### 1. Data Inspection\n",
    "\n",
    "- **Check for Missing Values:** Identify any missing or null entries across all features. Depending on the volume and pattern of missingness, decide whether to drop, impute (replace), or model around them.\n",
    "- **Data Types Check:** Review the data types of each feature (e.g., integer, float, object) to ensure they align with the expected format. For example, date columns should be in datetime format, and categorical variables should be strings or category types.\n",
    "- **Correlation Analysis:** Examine the correlation between independent variables and the target variable. This helps identify potentially predictive features and detect multicollinearity issues.\n",
    "\n",
    "### 2. Feature Exploration & Engineering\n",
    "\n",
    "- **Date Handling:** If the dataset includes date-time fields, I extract meaningful features such as year, month, day, week of year, day of week, etc. These engineered features can significantly enhance model performance by capturing temporal patterns.\n",
    "- **Missing Value Treatment:**\n",
    "  - Numerical Features (Continuous): Impute missing values using mean, median, or advanced methods like KNN or model-based imputation.\n",
    "  - Categorical Features: Replace missing values with \"Unknown\" or use frequency-based approaches.\n",
    "- **Normalization / Scaling:** Apply normalization or standardization techniques to numerical features when working with models sensitive to feature scales (e.g., SVMs, neural networks, gradient descent-based algorithms).\n",
    "- **Categorification (Encoding):** Convert categorical variables into numerical formats suitable for machine learning models:\n",
    "  - **One-Hot Encoding:** Preferred for nominal categorical variables with no inherent order (especially when cardinality is low).\n",
    "  - **Label Encoding:** Used for binary or ordinal variables where natural ordering exists. e.g.: `[\"small\", \"medium\", \"hard\"]`\n",
    "  - **Target Encoding / Leave-One-Out Encoding:** Useful for high-cardinality categorical features (like zip codes), where one-hot encoding would lead to dimensionality explosion. (Leave-One-Out Encoding replaces each category with the average target value of all other rows in the same category, excluding the current row. This helps prevent overfitting by avoiding data leakage during encoding.)\n",
    "\n",
    "### 3. Categorical vs Continuous Variables\n",
    "\n",
    "- **Categorical Variables:** These typically have object or string data types and represent discrete categories. They may require encoding before modeling.\n",
    "- **Continuous Variables:** These are numeric features that can take any value within a range. These usually undergo scaling or binning (discrete intervals or bins) depending on the model requirements.\n",
    "\n",
    "### 4. Cardinality Consideration\n",
    "\n",
    "- **High Cardinality:** Features like `zip_code`, `user_id`, or `product_id` that have thousands of unique values. These need special handling‚Äîsuch as grouping rare categories, hashing, or using embedding layers in deep learning.\n",
    "- **Low Cardinality:** Features with a small number of distinct values. These are generally easier to encode using one-hot or label encoding methods.\n",
    "\n",
    "### 5. Ordinal Variables\n",
    "\n",
    "Some categorical variables have a natural order, such as `\"easy\"`, `\"medium\"`, `\"hard\"` or `\"low\"`, `\"medium\"`, `\"high\"`. These are called **ordinal variables**, and their order must be preserved during encoding (e.g., via custom mapping or ordinal encoding).\n",
    "\n",
    "\n",
    "### üå≥ Decision Trees\n",
    "\n",
    "### What is a Decision Tree?\n",
    "\n",
    "A **decision tree** is a machine learning model that makes predictions by learning simple decision rules inferred from the data features. It splits the data into subsets based on feature values, forming a tree-like structure where:\n",
    "\n",
    "- Each **internal node** represents a test on a feature.\n",
    "- Each **branch** represents the outcome of that test.\n",
    "- Each **leaf node** represents a final prediction or class label (in classification) or value (in regression).\n",
    "\n",
    "### How Does a Decision Tree Work?\n",
    "\n",
    "1. **Recursive Partitioning**: The algorithm starts at the root of the tree and tries to split the data into two groups such that the resulting groups are as \"pure\" as possible with respect to the target variable.\n",
    "2. **Splitting Criteria**:\n",
    "   - For **regression trees**, it uses metrics like **Mean Squared Error (MSE)** or **Mean Absolute Error (MAE)**.\n",
    "   - For **classification trees**, it uses measures like **Gini impurity** or **Information Gain (based on entropy)**.\n",
    "    - (**Gini impurity** is a measure of how often a randomly chosen element from a set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in that subset).\n",
    "    - (**Information Gain** (IG) measures how much \"information\" a feature provides about the class labels. In other words, it tells us how well a feature separates the data into classes.)\n",
    "3. This process continues recursively until a stopping criterion is met (e.g., maximum depth, minimum number of samples per leaf).\n",
    "\n",
    "### Pros & Cons of Decision Trees\n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| Easy to interpret and visualize | Prone to overfitting |\n",
    "| Handles both numerical and categorical data well | Unstable: small changes in data can lead to very different trees |\n",
    "| No need for feature scaling | May not generalize well |\n",
    "\n",
    "### Ensemble Learning: Bagging & Random Forests\n",
    "\n",
    "### What is an Ensemble Method?\n",
    "\n",
    "An **ensemble method** combines multiple models to improve performance and reduce variance and bias.\n",
    "\n",
    "### üß∫ Bagging (Bootstrap Aggregating)\n",
    "\n",
    "**Bagging** works by:\n",
    "\n",
    "1. Randomly sampling subsets of the training data **with replacement** (called bootstrap samples).\n",
    "2. Training a base model (usually a decision tree) on each subset.\n",
    "3. Averaging predictions (for regression) or using majority voting (for classification) across all models.\n",
    "\n",
    "> This reduces variance and helps prevent overfitting.\n",
    "\n",
    "### üå≤ Random Forests\n",
    "\n",
    "**Random Forests** are an extension of bagging with one key difference:\n",
    "- At each split in the tree, only a **random subset of features** is considered for splitting.\n",
    "\n",
    "This further **decorrelates** the trees, improving performance.\n",
    "\n",
    "### Why Are Random Forests Effective?\n",
    "\n",
    "- Each tree sees slightly different data and features.\n",
    "- Errors from individual trees tend to cancel out when averaged.\n",
    "- They're robust to noise, outliers, and overfitting.\n",
    "\n",
    "### Out-of-Bag (OOB) Error\n",
    "\n",
    "### What is OOB Error?\n",
    "\n",
    "In random forests, since each tree is trained on a bootstrapped sample (~63% of the data), the remaining ~37% of rows **not used in training** a particular tree are called **Out-of-Bag (OOB)** samples.\n",
    "\n",
    "### Why Is OOB Error Useful?\n",
    "\n",
    "- You can use these OOB samples to estimate how well the model performs **without needing a separate validation set**.\n",
    "- For each row, you average predictions only from trees that did **not** include that row during training.\n",
    "\n",
    "üß† **Intuition**: OOB error is like having a built-in cross-validation mechanism for random forests.\n",
    "\n",
    "### Model Interpretation\n",
    "\n",
    "Even though ensemble models like random forests are more complex than single decision trees, they still allow us to understand **how** and **why** they make their predictions.\n",
    "\n",
    "### 1. **Feature Importance**\n",
    "\n",
    "- Measures how much each feature contributes to reducing uncertainty (or error) in predictions.\n",
    "- Computed by averaging the reduction in impurity (like Gini or MSE) brought by each feature across all trees.\n",
    "\n",
    "üéØ Use Case: Identify which features are most useful‚Äîdrop irrelevant ones or focus on them for domain insights.\n",
    "\n",
    "### 2. **Finding Out-of-Domain Data**\n",
    "\n",
    "- Train a classifier to distinguish between training and test sets.\n",
    "- If the classifier performs well, it means the test set differs significantly from the training set.\n",
    "- Helps identify **distribution shifts** that may affect generalization.\n",
    "\n",
    "### 3. **Partial Dependence Plots (PDP)**\n",
    "\n",
    "- Shows how a feature affects predictions **on average**, holding other features constant.\n",
    "- Reveals non-linear relationships and interactions.\n",
    "\n",
    "### 4. **Individual Conditional Expectation (ICE) Plots**\n",
    "\n",
    "- Like PDP but shows the effect for **individual data points** rather than averages.\n",
    "- Helps detect heterogeneity in effects.\n",
    "\n",
    "### 5. **Tree Interpreter / SHAP Values**\n",
    "\n",
    "- Explains individual predictions by decomposing the contribution of each feature.\n",
    "- Uses techniques like **SHapley Additive exPlanations (SHAP)** to fairly attribute prediction changes to input features.\n",
    "\n",
    "üéØ Use Case: Explain why a specific prediction was made‚Äîfor transparency, fairness, or debugging.\n",
    "\n",
    "### Ensembling Techniques\n",
    "\n",
    "### 1. **Bagging (as discussed above)**\n",
    "\n",
    "- Combines many models trained on different subsets of data.\n",
    "- Reduces variance ‚Üí improves generalization.\n",
    "\n",
    "### 2. **Boosting**\n",
    "\n",
    "- Sequentially trains weak learners, each correcting the errors of its predecessor.\n",
    "- Final prediction is a **weighted sum** of all models' predictions.\n",
    "\n",
    "### Popular Boosting Algorithms:\n",
    "\n",
    "- **AdaBoost**\n",
    "- **Gradient Boosted Trees (GBDT)**\n",
    "- **XGBoost**, **LightGBM**, **CatBoost**\n",
    "\n",
    "### Key Idea Behind Boosting:\n",
    "\n",
    "1. Train a simple model (often shallow trees).\n",
    "2. Compute residuals (errors).\n",
    "3. Train a new model to predict those residuals.\n",
    "4. Repeat and add corrections iteratively.\n",
    "\n",
    "üß† **Intuition**: Boosting learns slowly and focuses on hard-to-predict cases.\n",
    "\n",
    "### Bagging vs. Boosting\n",
    "\n",
    "| Feature | Bagging (e.g., Random Forest) | Boosting (e.g., XGBoost) |\n",
    "|--------|-------------------------------|---------------------------|\n",
    "| Training Style | Parallel | Sequential |\n",
    "| Focus | Reduce variance | Reduce bias |\n",
    "| Overfitting | Less prone | More prone if not tuned |\n",
    "| Speed | Faster to train | Slower due to sequential steps |\n",
    "| Performance | Strong baseline | Often higher accuracy with tuning |\n",
    "\n",
    "### Summary: When to Use What?\n",
    "\n",
    "- **Use Decision Trees** for interpretable, fast baselines.\n",
    "- **Use Random Forests (Bagging)** when you want good performance with less tuning.\n",
    "- **Use Boosting** when you want high performance and can afford more tuning and time.\n",
    "- **Use OOB error** to validate without a separate validation set.\n",
    "- **Use Feature Importance / SHAP / PDP** to understand and explain your model.\n",
    "\n",
    "### Model Evaluation Metrics\n",
    "\n",
    "Model evaluation is the process of assessing how well a machine learning model performs on unseen data. There are different metrics for classification and regression tasks.\n",
    "\n",
    "### ‚úÖ Classification Metrics\n",
    "\n",
    "| Metric | Description | Formula / Use Case | Function |\n",
    "|-------|-------------|---------------------|----------|\n",
    "| **Accuracy** | % of total correct predictions (both true positives and true negatives) | `TP + TN / TP + FP + TN + FN` | `accuracy_score()` |\n",
    "| **Precision** | How many selected items are relevant? | `TP / (TP + FP)` | `precision_score()` |\n",
    "| **Recall (Sensitivity)** | How many relevant items were selected? | `TP / (TP + FN)` | `recall_score()` |\n",
    "| **F1 Score** | Harmonic mean of precision and recall | `2 * (precision * recall)/(precision + recall)` | `f1_score()` |\n",
    "| **Confusion Matrix** | Table showing counts of true positives, false positives, true negatives, false negatives | Visual summary of performance | `confusion_matrix()` |\n",
    "| **ROC-AUC** | Area under the ROC curve; measures model‚Äôs ability to distinguish between classes | Higher AUC = better performance | `roc_auc_score()` |\n",
    "\n",
    ":::{.callout-note}\n",
    "üìå Use **ROC-AUC** when dealing with imbalanced datasets.  \n",
    "For class imbalance, prefer **F1 score** over accuracy.\n",
    ":::\n",
    "\n",
    "### üìà Regression Metrics\n",
    "\n",
    "| Metric | Description | Formula / Use Case | Function |\n",
    "|-------|-------------|---------------------|----------|\n",
    "| **Mean Absolute Error (MAE)** | Average of absolute errors | Mean of |actual - predicted|| | `mean_absolute_error()` |\n",
    "| **Mean Squared Error (MSE)** | Average of squared errors | Mean of (actual - predicted)¬≤ | `mean_squared_error()` |\n",
    "| **R¬≤ Score (R-squared)** | Proportion of variance explained by the model | Best possible score is 1.0 | `r2_score()` |\n",
    "\n",
    "> üìå MAE is more interpretable; MSE penalizes large errors more heavily.  \n",
    "> R¬≤ tells how well your model fits the data compared to a baseline.\n",
    "\n",
    "### üîß Hyperparameter Tuning\n",
    "\n",
    "Hyperparameters are settings that control how models learn. Unlike model parameters (like weights), hyperparameters are set before training.\n",
    "\n",
    "### Techniques:\n",
    "\n",
    "1. **Grid Search**\n",
    "   - Tries every combination of given hyperparameters.\n",
    "   - Exhaustive but slow if too many parameters.\n",
    "   - Use: When you have few parameters or want exhaustive search.\n",
    "\n",
    "2. **Random Search**\n",
    "   - Randomly samples combinations from specified ranges.\n",
    "   - Often faster than Grid Search and sometimes finds better results.\n",
    "\n",
    "3. **Bayesian Optimization**\n",
    "   - Uses probabilistic models to choose next parameter set.\n",
    "   - Efficient for high-dimensional spaces.\n",
    "   - Requires external libraries like `scikit-optimize`.\n",
    "\n",
    "### üîÅ Cross-validation\n",
    "\n",
    "Cross-validation helps estimate how well your model will perform on unseen data by evaluating it on multiple random splits of the dataset.\n",
    "\n",
    "### K-Fold Cross-Validation:\n",
    "\n",
    "- Splits data into K parts (folds).\n",
    "- Trains on K-1 folds, tests on 1 fold ‚Äî repeats K times.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Ensemble Methods\n",
    "\n",
    "Ensemble methods combine predictions from multiple models to improve performance.\n",
    "\n",
    "### Types of Ensemble Learning:\n",
    "\n",
    "### 1. **Bagging (Bootstrap Aggregating)**\n",
    "\n",
    "- Builds multiple models on bootstrapped samples of the data.\n",
    "- Final prediction is average (regression) or majority vote (classification).\n",
    "- Reduces variance.\n",
    "- Example: `BaggingClassifier()`, `RandomForestClassifier()`\n",
    "\n",
    "### 2. **Boosting**\n",
    "\n",
    "- Sequentially trains models to correct errors of previous models.\n",
    "- Focuses on hard-to-predict instances.\n",
    "- Reduces bias.\n",
    "- Examples:\n",
    "  - `AdaBoostClassifier()`\n",
    "  - `GradientBoostingClassifier()`\n",
    "  - Popular: `XGBoost`, `LightGBM`, `CatBoost`\n",
    "\n",
    "### 3. **Voting**\n",
    "\n",
    "- Combines predictions from multiple base classifiers.\n",
    "- Hard Voting: Majority class.\n",
    "- Soft Voting: Weighted probabilities.\n",
    "\n",
    "### 4. **Stacking**\n",
    "\n",
    "- Train a \"meta-model\" to combine outputs of base models.\n",
    "- Base models' predictions become new features for the meta-model.\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Topic | Key Functions/Classes | Purpose |\n",
    "|-------|------------------------|---------|\n",
    "| Model Evaluation | `accuracy_score`, `precision_score`, etc. | Assess model performance |\n",
    "| Hyperparameter Tuning | `GridSearchCV`, `RandomizedSearchCV` | Find best model settings |\n",
    "| Feature Engineering | `StandardScaler`, `OneHotEncoder`, `SimpleImputer` | Improve input quality |\n",
    "| Pipelines | `Pipeline` | Automate preprocessing + modeling |\n",
    "| Cross-validation | `cross_val_score` | Estimate generalization performance |\n",
    "| Ensemble Methods | `BaggingClassifier`, `GradientBoostingClassifier`, `VotingClassifier`, `StackingClassifier` | Boost performance via combining models |\n",
    "\n",
    "### What is Clustering?\n",
    "\n",
    "**Clustering** is an **unsupervised machine learning technique** used to group similar data points together into clusters. It helps find patterns or structures in data **without prior knowledge of the groups** ‚Äî unlike classification, where we already know the categories.\n",
    "\n",
    "### Why Do We Use Clustering?\n",
    "\n",
    "Clustering helps answer questions like:\n",
    "\n",
    "- How many distinct groups exist in my data?\n",
    "- Are there any unusual or outlier observations?\n",
    "- Can I simplify or summarize the data by grouping similar items?\n",
    "\n",
    "Common use cases include:\n",
    "\n",
    "- Customer segmentation\n",
    "- Image compression\n",
    "- Document grouping\n",
    "- Anomaly detection\n",
    "- Social network analysis\n",
    "\n",
    "### Types of Clustering Algorithms\n",
    "\n",
    "Here are some popular clustering algorithms:\n",
    "\n",
    "### 1. **K-Means Clustering**\n",
    "\n",
    "- Groups data into `k` number of clusters.\n",
    "- Starts with random centers and iteratively assigns points to the nearest cluster center.\n",
    "- Goal: Minimize the sum of squared distances between points and their cluster centers.\n",
    "\n",
    "‚úÖ Pros: Simple, fast  \n",
    "‚ùå Cons: Needs `k` specified, assumes spherical clusters\n",
    "\n",
    "### 2. **Hierarchical Clustering**\n",
    "\n",
    "- Builds a tree-like structure (dendrogram) showing how clusters merge or split.\n",
    "- Two types:\n",
    "  - **Agglomerative** (bottom-up): starts with individual points, merges them.\n",
    "  - **Divisive** (top-down): starts with all points in one cluster, splits recursively.\n",
    "\n",
    "‚úÖ Pros: No need to specify number of clusters  \n",
    "‚ùå Cons: Computationally expensive for large datasets\n",
    "\n",
    "### 3. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**\n",
    "\n",
    "- Groups points that are close together and marks outliers as noise.\n",
    "- Doesn‚Äôt require specifying number of clusters.\n",
    "\n",
    "‚úÖ Pros: Handles noise well, finds arbitrarily shaped clusters  \n",
    "‚ùå Cons: Sensitive to parameter settings\n",
    "\n",
    "### 4. **Gaussian Mixture Models (GMMs)**\n",
    "\n",
    "- Assumes data comes from a mixture of Gaussian distributions.\n",
    "- Uses probabilities to assign points to clusters.\n",
    "\n",
    "‚úÖ Pros: Soft clustering (gives probability of belonging to each cluster)  \n",
    "‚ùå Cons: Slower than K-Means\n",
    "\n",
    "### How Does Clustering Work? (Simplified)\n",
    "\n",
    "Let‚Äôs take **K-Means** as an example:\n",
    "\n",
    "1. Choose the number of clusters `k`.\n",
    "2. Randomly place `k` centroids (center points).\n",
    "3. Assign each data point to the nearest centroid.\n",
    "4. Recalculate centroids based on the mean of assigned points.\n",
    "5. Repeat steps 3‚Äì4 until centroids stabilize.\n",
    "\n",
    "### Evaluating Clusters\n",
    "\n",
    "There's no \"right\" answer in unsupervised learning, but you can still evaluate quality using metrics like:\n",
    "\n",
    "- **Silhouette Score**: Measures how similar a point is to its own cluster vs others (ranges from -1 to +1)\n",
    "- **Elbow Method**: Helps choose optimal `k` by plotting sum of squared distances vs. number of clusters\n",
    "- **Davies-Bouldin Index**, **Calinski-Harabasz Index**, etc.\n",
    "\n",
    "### Real-Life Example\n",
    "\n",
    "Imagine you're a marketing analyst and want to segment customers based on spending habits:\n",
    "\n",
    "| Customer | Annual Income | Spending Score |\n",
    "|----------|----------------|----------------|\n",
    "| A        | 50             | 40             |\n",
    "| B        | 60             | 60             |\n",
    "| C        | 10             | 90             |\n",
    "| D        | 80             | 20             |\n",
    "\n",
    "Using clustering, you might discover:\n",
    "\n",
    "- Group 1: High income, high spenders\n",
    "- Group 2: Low income, high spenders\n",
    "- Group 3: High income, low spenders\n",
    "\n",
    "Each group can be targeted with different marketing strategies.\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Feature                | Clustering                        |\n",
    "|------------------------|-----------------------------------|\n",
    "| Type                   | Unsupervised                      |\n",
    "| Input                  | Data without labels               |\n",
    "| Output                 | Groups/clusters of similar points |\n",
    "| Popular Algorithms     | K-Means, DBSCAN, Hierarchical     |\n",
    "| Evaluation Metrics     | Silhouette score, Elbow method    |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
