[
  {
    "objectID": "posts/git_github/git_github.html",
    "href": "posts/git_github/git_github.html",
    "title": "Understanding the Git and Github (Everything you need to know to get started)",
    "section": "",
    "text": "Version Control System simply means we can go the back to version we want to. There are Various version Control system available such as git, CVS, Subversion, Perforce, Bazaar, and so on.\n\n\n\n\n\nflowchart LR\nZ[v0]--&gt;A[v1]--&gt;B(v2)--&gt;C(v3)\nmaster[Your here] --&gt;C\n\n\n\n\n\n\nWe are at the version 3 we can go back to we version 2 and made the changes come back. It will be very usefull when we collaborating with many people.\n\n\n\n\n\nflowchart LR\nZ[v0]--&gt;A[v1]--&gt;B(v2)--&gt;C(v3)\nmaster[Your here] --&gt;B"
  },
  {
    "objectID": "posts/git_github/git_github.html#version-control-system",
    "href": "posts/git_github/git_github.html#version-control-system",
    "title": "Understanding the Git and Github (Everything you need to know to get started)",
    "section": "",
    "text": "Version Control System simply means we can go the back to version we want to. There are Various version Control system available such as git, CVS, Subversion, Perforce, Bazaar, and so on.\n\n\n\n\n\nflowchart LR\nZ[v0]--&gt;A[v1]--&gt;B(v2)--&gt;C(v3)\nmaster[Your here] --&gt;C\n\n\n\n\n\n\nWe are at the version 3 we can go back to we version 2 and made the changes come back. It will be very usefull when we collaborating with many people.\n\n\n\n\n\nflowchart LR\nZ[v0]--&gt;A[v1]--&gt;B(v2)--&gt;C(v3)\nmaster[Your here] --&gt;B"
  },
  {
    "objectID": "posts/git_github/git_github.html#git-and-github",
    "href": "posts/git_github/git_github.html#git-and-github",
    "title": "Understanding the Git and Github (Everything you need to know to get started)",
    "section": "Git and Github",
    "text": "Git and Github\nWe are going to see Git and github(largest host for git repository)\n\nThree states of git\n\nModified (Files you are Modified)\nstaged (Modified file are taked out to store in local database)\ncommit (Staged files are stored in local database)\n\n\n\nInstallation\n\nInstall the git based on your operating system.\nCreate a github account.\n\nIt is Straight forward, if you are having problem in installing feel free to ask.\n\nInitial Config\n$ git config --global user.name \"Your Name\"\n$ git config --global user.email yourname@example.com\nYou can also config your editor, if you just starting out start with notepad for better understanding of concepts."
  },
  {
    "objectID": "posts/git_github/git_github.html#git-basics",
    "href": "posts/git_github/git_github.html#git-basics",
    "title": "Understanding the Git and Github (Everything you need to know to get started)",
    "section": "Git Basics",
    "text": "Git Basics\nIf your are beginner feel free to follow along with me. Once your comfortable you can create your own repository and test it out what you are learned. repository(repo) - Nothing but a folder\n\nBasic Commands\n\nls -&gt; list the file or folder in that region\nls -a -&gt;(list the hidden files) it work on the linux\nmkdir  -&gt; create a new folder\ncd -&gt; change the directory or get in to the directory\nrm -&gt; , rm -r  recursively remove everything\n\n\n\nGit Initialization, stage, commit\n\nGit Initialization\n\nmkdir understand_git -&gt; Create a folder with name understand_git\ncd understand_git -&gt; get into the understand_git folder\ngit init -&gt; initialize the git in the folder\n\nls -a -&gt; if you want to look out what what it initialized is .git(It work on linux)\n\n\n\n\nGit stage\n\nCreate a README.md file in the repo. And write a sentence like this is about understanding git.\ngit status -&gt; Check the status of file. It shows on master branch and README.md file as untracked file.\ngit add . or git add * (If you want to particular file -&gt; git add README.md)\n\n\n\nGit Commit\n\nCheck the status(git status) untracked file become become tracked(File colour changed from red to green)\ngit commit -m ‚ÄúInital commit‚Äù (-m is the message for our local storage in case we want to go back it will be very usefull)\ngit status -&gt; Branch is clean nothing to commit\n\ngit status -s (Short status for the track)\n\n\n\n\n\nIgnoring files/folder\nThere are few file/folder like cache, log, etc. We don‚Äôt need to maintain history or track for that type of file and we can ignore from tracking by creating .gitignore file and add the name of the file or folder to ignore. We can also use Regular expression to ignore file\n\n\nStaged and Unstaged changes\n\nlet‚Äôs open a README.md file and add the second line like git is a powerfull tool\ngit status will show the file is modified\ngit diff -&gt; Shows the what is modified from the last commit\ngit diff ‚Äìstaged or git diff ‚Äìcached -&gt; What are change in the staged file difference from the previous commit\n\n\n\nSkipping the staging area.\n\nIf you also skip the staging area and directly stage and commit\n\nAdd a line in README.md like github is amazing.\ngit commit -am ‚ÄúLet‚Äôs skip the stagging area‚Äù or git commit -a -m ‚ÄúLet‚Äôs skip the stagging area‚Äù.\nUse it carefully sometime we don‚Äôt want to add every changes in every file.\n\n\n\n\nUnstage the staged file\n\nIf you are accidently staged some file and want to unstage the file\n\ncreate a text file called file_1.txt and add a line like nature is amazing\ngit add . -&gt; it adds the file to the staging area and you accidently added that file for staging\ngit rm ‚Äìcached file_1.txt -&gt; it will unstage the file_1.txt\nyou can also do it by (git reset file_1.txt)\n\n\n\n\nLog\n\ngit log -&gt; Gives the commit history(It is very much usefull when we want to roll back)\ngit log ‚Äìstat -&gt; Gives the commit history with changes\n\n\n\nUndoing the commit message\n\nIf you mess up with your commit message you can go back and edit the message.\nOpen README.md add text in a new line like i love programming\ngit add .\ngit commit -m ‚Äúadded‚Äù\nyou just added the message as added so i want message as added a new line\ngit commit ‚Äìamend -&gt; It will open message prompt to re-enter your commit message.\n\n\n\nUnmodifying a modified file\n\nOpen README.md add text in a new line like Python is great\nCheck the git status, README.md is modified\ngit checkout ‚Äì README.md -&gt; it will roll back the modified file.\n\n\n\nGit restore\n\ngit restore ‚Äìstaged file_name -&gt; Unstage the staged file\ngit restore file_name -&gt; Unmodifying the modified file\n\n\nWhat to use?\n\ngit reset or git cached or git restore - I will suggest git restore because it is powerfull than others"
  },
  {
    "objectID": "posts/git_github/git_github.html#git-branching-and-github",
    "href": "posts/git_github/git_github.html#git-branching-and-github",
    "title": "Understanding the Git and Github (Everything you need to know to get started)",
    "section": "Git Branching and Github",
    "text": "Git Branching and Github\n\nGithub Intro\n\nGithub largest host of git repo- Hope you already created that account if your are following this.\nFor practice purpose i will create a understanding_git repo in my github you can fork it follow along and i will merge your pull request.\nGithub uses main instead of master branch from few year back you can also configure from name as main/master\n\n\n\nGit branching\nBefore deep diving in to the branches let understand few basics. - You are not able to directly commit on the project of the team/Organization. - First you need a copy of the project in your repo just click the fork of this project at the right side here. - Once it forked copy the url from the code dropdown or repo url. - git clone url -&gt; This command will copy the repo in the local.\n\n\n\n\n\n\nCreate a new branch Once cloned\n\n\n\n\nNever ever commit on the master/main branch\nNew branch new issue\n\n\n\n\n\nContribution to a project\n\nI test the every command so anything not working create an issue here\ngit remote upstream https://github.com/MUKESHRAJMAHENDRAN/understanding_git (This will add origin of the forked repo)\ngit branch inspired_line -&gt; Create a new branch\ngit checkout inspired_line\nadd your line on the quote.txt\ngit add .\ngit commit -m ‚ÄúAdded the line‚Äù\ngit checkout main\ngit pull upstream main -&gt; If any change are commited on the parent main branch that with sync with main branch of the local\ngit merge inspired_line (It open message prompt and type what merge message is all about)\ngit push origin main\nOnce pushed go to your repo of understanding_git from the contribution create a open pull request i will merge it\nOnce merged delete the branch name\ngit branch -d inspired_line\n\nThis blog is just a fuel to kick to get starting there are so much things to learn from git and github like Git Rebase, merge conflict, cherry pick, git on the server, distribution of git, branch Workflow, etc.We will learn in the upcomming post."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicting the future isn‚Äôt magic, it‚Äôs Artificial Intelligence.",
    "section": "",
    "text": "Brush up your Machine learning Skill\n\n\n\nMachine leaning\n\nEDA\n\nAnalysis\n\nPreprocessing\n\nPrediction\n\nPipeline\n\n\n\n\n\n\n\n\n\nJun 18, 2025\n\n\nMukesh\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Git and Github (Everything you need to know to get started)\n\n\n\nVersion Control System\n\nDev\n\nData Science\n\nGit\n\nCode\n\n\n\n\n\n\n\n\n\nJun 11, 2025\n\n\nMukesh\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hi üëã I‚Äôm Mukesh",
    "section": "",
    "text": "üöÄ About Me\nüíº Current: AI Engineer at Sena\nüîô Past: Data Scientist at Cubera and Visionet\nüìù Blogger at DataSherlock, writing about: topics = [\"Python\", \"Data Exploration\", \"Visualization\", \"Feature Engineering\", \"Machine Learning\", \"Deep Learning\"]\nüèÖ Kaggle Notebook Expert: Active on Kaggle, constantly learning and contributing to the data science community.\nüí¨ Open for Questions: Reach out or start a discussion here ‚Äî happy to connect!\n‚ù§Ô∏è What Drives Me: I enjoy writing clean code, learning new technologies, and building meaningful projects. I‚Äôm also passionate about contributing to open source ‚Äî including libraries like Narwhals.\nüåê Currently building my own website with quarto. This site is for person like me who lost in abundance of knowlege in the internet for learning data science."
  },
  {
    "objectID": "posts/tablur_data_revision/machine_leaning.html",
    "href": "posts/tablur_data_revision/machine_leaning.html",
    "title": "Brush up your Machine learning Skill",
    "section": "",
    "text": "Machine learning\n\nIn a simple terms computer is able to learn from the data without being explicitly programmed.\n\n\n\n\n\n\n\nTip\n\n\n\nAny sort of data science work Look at your data directly even though you have an description.\n\n\n\nTypes of Machine Learning\n\n\n\n\n\n\n\n\nType\nDescription\nExamples\n\n\n\n\nSupervised Learning\nUses labeled data (input + output)\nClassification, Regression\n\n\nUnsupervised Learning\nUses unlabeled data (only input)\nClustering, Dimensionality Reduction\n\n\nSemi-supervised Learning\nMix of labeled and unlabeled data\nImage classification with limited labels\n\n\nReinforcement Learning\nAgent learns by trial and error with rewards/penalties\nGame AI, Robotics\n\n\n\n\n\nKey Concepts in Machine Learning\n\n\n\n\n\n\n\nConcept\nExplanation\n\n\n\n\nFeatures (X) or Independent variable\nInput variables used to make predictions\n\n\nTarget (y) or Dependent variable\nOutput variable we want to predict\n\n\nTraining Data\nData used to train the model\n\n\nTest Data\nData used to evaluate the model\n\n\nOverfitting\nModel performs well on training data but poorly on new data\n\n\nUnderfitting\nModel doesn‚Äôt perform well even on training data\n\n\nBias-Variance Tradeoff\nBalancing simplicity vs complexity of a model\n\n\nGeneralization\nHow well the model performs on unseen data\n\n\n\n\n\nData Understanding & Preprocessing\nWhen I first receive the dataset, I begin by performing an initial assessment to understand its structure and quality. This involves the following key steps:\n\n\n1. Data Inspection\n\nCheck for Missing Values: Identify any missing or null entries across all features. Depending on the volume and pattern of missingness, decide whether to drop, impute (replace), or model around them.\nData Types Check: Review the data types of each feature (e.g., integer, float, object) to ensure they align with the expected format. For example, date columns should be in datetime format, and categorical variables should be strings or category types.\nCorrelation Analysis: Examine the correlation between independent variables and the target variable. This helps identify potentially predictive features and detect multicollinearity issues.\n\n\n\n2. Feature Exploration & Engineering\n\nDate Handling: If the dataset includes date-time fields, I extract meaningful features such as year, month, day, week of year, day of week, etc. These engineered features can significantly enhance model performance by capturing temporal patterns.\nMissing Value Treatment:\n\nNumerical Features (Continuous): Impute missing values using mean, median, or advanced methods like KNN or model-based imputation.\nCategorical Features: Replace missing values with ‚ÄúUnknown‚Äù or use frequency-based approaches.\n\nNormalization / Scaling: Apply normalization or standardization techniques to numerical features when working with models sensitive to feature scales (e.g., SVMs, neural networks, gradient descent-based algorithms).\nCategorification (Encoding): Convert categorical variables into numerical formats suitable for machine learning models:\n\nOne-Hot Encoding: Preferred for nominal categorical variables with no inherent order (especially when cardinality is low).\nLabel Encoding: Used for binary or ordinal variables where natural ordering exists. e.g.: [\"small\", \"medium\", \"hard\"]\nTarget Encoding / Leave-One-Out Encoding: Useful for high-cardinality categorical features (like zip codes), where one-hot encoding would lead to dimensionality explosion. (Leave-One-Out Encoding replaces each category with the average target value of all other rows in the same category, excluding the current row. This helps prevent overfitting by avoiding data leakage during encoding.)\n\n\n\n\n3. Categorical vs Continuous Variables\n\nCategorical Variables: These typically have object or string data types and represent discrete categories. They may require encoding before modeling.\nContinuous Variables: These are numeric features that can take any value within a range. These usually undergo scaling or binning (discrete intervals or bins) depending on the model requirements.\n\n\n\n4. Cardinality Consideration\n\nHigh Cardinality: Features like zip_code, user_id, or product_id that have thousands of unique values. These need special handling‚Äîsuch as grouping rare categories, hashing, or using embedding layers in deep learning.\nLow Cardinality: Features with a small number of distinct values. These are generally easier to encode using one-hot or label encoding methods.\n\n\n\n5. Ordinal Variables\nSome categorical variables have a natural order, such as \"easy\", \"medium\", \"hard\" or \"low\", \"medium\", \"high\". These are called ordinal variables, and their order must be preserved during encoding (e.g., via custom mapping or ordinal encoding).\n\n\nüå≥ Decision Trees\n\n\nWhat is a Decision Tree?\nA decision tree is a machine learning model that makes predictions by learning simple decision rules inferred from the data features. It splits the data into subsets based on feature values, forming a tree-like structure where:\n\nEach internal node represents a test on a feature.\nEach branch represents the outcome of that test.\nEach leaf node represents a final prediction or class label (in classification) or value (in regression).\n\n\n\nHow Does a Decision Tree Work?\n\nRecursive Partitioning: The algorithm starts at the root of the tree and tries to split the data into two groups such that the resulting groups are as ‚Äúpure‚Äù as possible with respect to the target variable.\nSplitting Criteria:\n\nFor regression trees, it uses metrics like Mean Squared Error (MSE) or Mean Absolute Error (MAE).\nFor classification trees, it uses measures like Gini impurity or Information Gain (based on entropy).\n(Gini impurity is a measure of how often a randomly chosen element from a set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in that subset).\n(Information Gain (IG) measures how much ‚Äúinformation‚Äù a feature provides about the class labels. In other words, it tells us how well a feature separates the data into classes.)\n\nThis process continues recursively until a stopping criterion is met (e.g., maximum depth, minimum number of samples per leaf).\n\n\n\nPros & Cons of Decision Trees\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nEasy to interpret and visualize\nProne to overfitting\n\n\nHandles both numerical and categorical data well\nUnstable: small changes in data can lead to very different trees\n\n\nNo need for feature scaling\nMay not generalize well\n\n\n\n\n\nEnsemble Learning: Bagging & Random Forests\n\n\nWhat is an Ensemble Method?\nAn ensemble method combines multiple models to improve performance and reduce variance and bias.\n\n\nüß∫ Bagging (Bootstrap Aggregating)\nBagging works by:\n\nRandomly sampling subsets of the training data with replacement (called bootstrap samples).\nTraining a base model (usually a decision tree) on each subset.\nAveraging predictions (for regression) or using majority voting (for classification) across all models.\n\n\nThis reduces variance and helps prevent overfitting.\n\n\n\nüå≤ Random Forests\nRandom Forests are an extension of bagging with one key difference: - At each split in the tree, only a random subset of features is considered for splitting.\nThis further decorrelates the trees, improving performance.\n\n\nWhy Are Random Forests Effective?\n\nEach tree sees slightly different data and features.\nErrors from individual trees tend to cancel out when averaged.\nThey‚Äôre robust to noise, outliers, and overfitting.\n\n\n\nOut-of-Bag (OOB) Error\n\n\nWhat is OOB Error?\nIn random forests, since each tree is trained on a bootstrapped sample (~63% of the data), the remaining ~37% of rows not used in training a particular tree are called Out-of-Bag (OOB) samples.\n\n\nWhy Is OOB Error Useful?\n\nYou can use these OOB samples to estimate how well the model performs without needing a separate validation set.\nFor each row, you average predictions only from trees that did not include that row during training.\n\nüß† Intuition: OOB error is like having a built-in cross-validation mechanism for random forests.\n\n\nModel Interpretation\nEven though ensemble models like random forests are more complex than single decision trees, they still allow us to understand how and why they make their predictions.\n\n\n1. Feature Importance\n\nMeasures how much each feature contributes to reducing uncertainty (or error) in predictions.\nComputed by averaging the reduction in impurity (like Gini or MSE) brought by each feature across all trees.\n\nüéØ Use Case: Identify which features are most useful‚Äîdrop irrelevant ones or focus on them for domain insights.\n\n\n2. Finding Out-of-Domain Data\n\nTrain a classifier to distinguish between training and test sets.\nIf the classifier performs well, it means the test set differs significantly from the training set.\nHelps identify distribution shifts that may affect generalization.\n\n\n\n3. Partial Dependence Plots (PDP)\n\nShows how a feature affects predictions on average, holding other features constant.\nReveals non-linear relationships and interactions.\n\n\n\n4. Individual Conditional Expectation (ICE) Plots\n\nLike PDP but shows the effect for individual data points rather than averages.\nHelps detect heterogeneity in effects.\n\n\n\n5. Tree Interpreter / SHAP Values\n\nExplains individual predictions by decomposing the contribution of each feature.\nUses techniques like SHapley Additive exPlanations (SHAP) to fairly attribute prediction changes to input features.\n\nüéØ Use Case: Explain why a specific prediction was made‚Äîfor transparency, fairness, or debugging.\n\n\nEnsembling Techniques\n\n\n1. Bagging (as discussed above)\n\nCombines many models trained on different subsets of data.\nReduces variance ‚Üí improves generalization.\n\n\n\n2. Boosting\n\nSequentially trains weak learners, each correcting the errors of its predecessor.\nFinal prediction is a weighted sum of all models‚Äô predictions.\n\n\n\nPopular Boosting Algorithms:\n\nAdaBoost\nGradient Boosted Trees (GBDT)\nXGBoost, LightGBM, CatBoost\n\n\n\nKey Idea Behind Boosting:\n\nTrain a simple model (often shallow trees).\nCompute residuals (errors).\nTrain a new model to predict those residuals.\nRepeat and add corrections iteratively.\n\nüß† Intuition: Boosting learns slowly and focuses on hard-to-predict cases.\n\n\nBagging vs.¬†Boosting\n\n\n\nFeature\nBagging (e.g., Random Forest)\nBoosting (e.g., XGBoost)\n\n\n\n\nTraining Style\nParallel\nSequential\n\n\nFocus\nReduce variance\nReduce bias\n\n\nOverfitting\nLess prone\nMore prone if not tuned\n\n\nSpeed\nFaster to train\nSlower due to sequential steps\n\n\nPerformance\nStrong baseline\nOften higher accuracy with tuning\n\n\n\n\n\nSummary: When to Use What?\n\nUse Decision Trees for interpretable, fast baselines.\nUse Random Forests (Bagging) when you want good performance with less tuning.\nUse Boosting when you want high performance and can afford more tuning and time.\nUse OOB error to validate without a separate validation set.\nUse Feature Importance / SHAP / PDP to understand and explain your model.\n\n\n\nModel Evaluation Metrics\nModel evaluation is the process of assessing how well a machine learning model performs on unseen data. There are different metrics for classification and regression tasks.\n\n\n‚úÖ Classification Metrics\n\n\n\n\n\n\n\n\n\nMetric\nDescription\nFormula / Use Case\nFunction\n\n\n\n\nAccuracy\n% of total correct predictions (both true positives and true negatives)\nTP + TN / TP + FP + TN + FN\naccuracy_score()\n\n\nPrecision\nHow many selected items are relevant?\nTP / (TP + FP)\nprecision_score()\n\n\nRecall (Sensitivity)\nHow many relevant items were selected?\nTP / (TP + FN)\nrecall_score()\n\n\nF1 Score\nHarmonic mean of precision and recall\n2 * (precision * recall)/(precision + recall)\nf1_score()\n\n\nConfusion Matrix\nTable showing counts of true positives, false positives, true negatives, false negatives\nVisual summary of performance\nconfusion_matrix()\n\n\nROC-AUC\nArea under the ROC curve; measures model‚Äôs ability to distinguish between classes\nHigher AUC = better performance\nroc_auc_score()\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nüìå Use ROC-AUC when dealing with imbalanced datasets.\nFor class imbalance, prefer F1 score over accuracy.\n\n\n\n\nüìà Regression Metrics\n\n\n\n\n\n\n\n\n\nMetric\nDescription\nFormula / Use Case\nFunction\n\n\n\n\nMean Absolute Error (MAE)\nAverage of absolute errors\nMean of\nactual - predicted\n\n\nMean Squared Error (MSE)\nAverage of squared errors\nMean of (actual - predicted)¬≤\nmean_squared_error()\n\n\nR¬≤ Score (R-squared)\nProportion of variance explained by the model\nBest possible score is 1.0\nr2_score()\n\n\n\n\nüìå MAE is more interpretable; MSE penalizes large errors more heavily.\nR¬≤ tells how well your model fits the data compared to a baseline.\n\n\n\nüîß Hyperparameter Tuning\nHyperparameters are settings that control how models learn. Unlike model parameters (like weights), hyperparameters are set before training.\n\n\nTechniques:\n\nGrid Search\n\nTries every combination of given hyperparameters.\nExhaustive but slow if too many parameters.\nUse: When you have few parameters or want exhaustive search.\n\nRandom Search\n\nRandomly samples combinations from specified ranges.\nOften faster than Grid Search and sometimes finds better results.\n\nBayesian Optimization\n\nUses probabilistic models to choose next parameter set.\nEfficient for high-dimensional spaces.\nRequires external libraries like scikit-optimize.\n\n\n\n\nüîÅ Cross-validation\nCross-validation helps estimate how well your model will perform on unseen data by evaluating it on multiple random splits of the dataset.\n\n\nK-Fold Cross-Validation:\n\nSplits data into K parts (folds).\nTrains on K-1 folds, tests on 1 fold ‚Äî repeats K times.\n\n\n\n\nüß† Ensemble Methods\nEnsemble methods combine predictions from multiple models to improve performance.\n\n\nTypes of Ensemble Learning:\n\n\n1. Bagging (Bootstrap Aggregating)\n\nBuilds multiple models on bootstrapped samples of the data.\nFinal prediction is average (regression) or majority vote (classification).\nReduces variance.\nExample: BaggingClassifier(), RandomForestClassifier()\n\n\n\n2. Boosting\n\nSequentially trains models to correct errors of previous models.\nFocuses on hard-to-predict instances.\nReduces bias.\nExamples:\n\nAdaBoostClassifier()\nGradientBoostingClassifier()\nPopular: XGBoost, LightGBM, CatBoost\n\n\n\n\n3. Voting\n\nCombines predictions from multiple base classifiers.\nHard Voting: Majority class.\nSoft Voting: Weighted probabilities.\n\n\n\n4. Stacking\n\nTrain a ‚Äúmeta-model‚Äù to combine outputs of base models.\nBase models‚Äô predictions become new features for the meta-model.\n\n\n\nSummary Table\n\n\n\n\n\n\n\n\nTopic\nKey Functions/Classes\nPurpose\n\n\n\n\nModel Evaluation\naccuracy_score, precision_score, etc.\nAssess model performance\n\n\nHyperparameter Tuning\nGridSearchCV, RandomizedSearchCV\nFind best model settings\n\n\nFeature Engineering\nStandardScaler, OneHotEncoder, SimpleImputer\nImprove input quality\n\n\nPipelines\nPipeline\nAutomate preprocessing + modeling\n\n\nCross-validation\ncross_val_score\nEstimate generalization performance\n\n\nEnsemble Methods\nBaggingClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\nBoost performance via combining models\n\n\n\n\n\nWhat is Clustering?\nClustering is an unsupervised machine learning technique used to group similar data points together into clusters. It helps find patterns or structures in data without prior knowledge of the groups ‚Äî unlike classification, where we already know the categories.\n\n\nWhy Do We Use Clustering?\nClustering helps answer questions like:\n\nHow many distinct groups exist in my data?\nAre there any unusual or outlier observations?\nCan I simplify or summarize the data by grouping similar items?\n\nCommon use cases include:\n\nCustomer segmentation\nImage compression\nDocument grouping\nAnomaly detection\nSocial network analysis\n\n\n\nTypes of Clustering Algorithms\nHere are some popular clustering algorithms:\n\n\n1. K-Means Clustering\n\nGroups data into k number of clusters.\nStarts with random centers and iteratively assigns points to the nearest cluster center.\nGoal: Minimize the sum of squared distances between points and their cluster centers.\n\n‚úÖ Pros: Simple, fast\n‚ùå Cons: Needs k specified, assumes spherical clusters\n\n\n2. Hierarchical Clustering\n\nBuilds a tree-like structure (dendrogram) showing how clusters merge or split.\nTwo types:\n\nAgglomerative (bottom-up): starts with individual points, merges them.\nDivisive (top-down): starts with all points in one cluster, splits recursively.\n\n\n‚úÖ Pros: No need to specify number of clusters\n‚ùå Cons: Computationally expensive for large datasets\n\n\n3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n\nGroups points that are close together and marks outliers as noise.\nDoesn‚Äôt require specifying number of clusters.\n\n‚úÖ Pros: Handles noise well, finds arbitrarily shaped clusters\n‚ùå Cons: Sensitive to parameter settings\n\n\n4. Gaussian Mixture Models (GMMs)\n\nAssumes data comes from a mixture of Gaussian distributions.\nUses probabilities to assign points to clusters.\n\n‚úÖ Pros: Soft clustering (gives probability of belonging to each cluster)\n‚ùå Cons: Slower than K-Means\n\n\nHow Does Clustering Work? (Simplified)\nLet‚Äôs take K-Means as an example:\n\nChoose the number of clusters k.\nRandomly place k centroids (center points).\nAssign each data point to the nearest centroid.\nRecalculate centroids based on the mean of assigned points.\nRepeat steps 3‚Äì4 until centroids stabilize.\n\n\n\nEvaluating Clusters\nThere‚Äôs no ‚Äúright‚Äù answer in unsupervised learning, but you can still evaluate quality using metrics like:\n\nSilhouette Score: Measures how similar a point is to its own cluster vs others (ranges from -1 to +1)\nElbow Method: Helps choose optimal k by plotting sum of squared distances vs.¬†number of clusters\nDavies-Bouldin Index, Calinski-Harabasz Index, etc.\n\n\n\nReal-Life Example\nImagine you‚Äôre a marketing analyst and want to segment customers based on spending habits:\n\n\n\nCustomer\nAnnual Income\nSpending Score\n\n\n\n\nA\n50\n40\n\n\nB\n60\n60\n\n\nC\n10\n90\n\n\nD\n80\n20\n\n\n\nUsing clustering, you might discover:\n\nGroup 1: High income, high spenders\nGroup 2: Low income, high spenders\nGroup 3: High income, low spenders\n\nEach group can be targeted with different marketing strategies.\n\n\nSummary\n\n\n\nFeature\nClustering\n\n\n\n\nType\nUnsupervised\n\n\nInput\nData without labels\n\n\nOutput\nGroups/clusters of similar points\n\n\nPopular Algorithms\nK-Means, DBSCAN, Hierarchical\n\n\nEvaluation Metrics\nSilhouette score, Elbow method"
  }
]